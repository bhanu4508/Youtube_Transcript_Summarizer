# -*- coding: utf-8 -*-
"""youtube_transcript_summerizer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LqI-2kLbGHAf4cu0qNYM56syFZdSgaiS
"""

!pip install transformers

import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

def gen_summary(text):
  # Sample text
  # text = """
  # Natural language processing (NLP) is a field of computer science that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of human language in a valuable way. NLP is used to apply machine learning algorithms to text and speech.
  # TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.
  # In NLP, text summarization is the process of distilling the most important information from a source (or sources) to create a shortened version while retaining the main ideas and key points.
  # There are two main approaches to text summarization: extractive and abstractive. Extractive methods select sentences from the source text, while abstractive methods generate new sentences to convey the summary.
  # """
  # Tokenize the input text
  input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)

  # Generate the summary
  summary_ids = model.generate(input_ids, max_length=100, num_return_sequences=1, length_penalty=2.0, early_stopping=True)

  # Decode the summary
  summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

  # Print the summary
  # print(summary)

  return summary

!pip install youtube_transcript_api
from youtube_transcript_api import YouTubeTranscriptApi
def get_transcript_of_yt_video(v_id):

    try:

        transcript_list = YouTubeTranscriptApi.list_transcripts(v_id)
        l = len(list(transcript_list))

        if (l > 1):
            try:
                final_transcript = YouTubeTranscriptApi.get_transcript(
                    v_id, languages=['en'])
                return final_transcript
            except:
                for i in transcript_list:
                    start_with = str(i)[:2]
                    transcript = transcript_list.find_transcript([start_with])
                    final_transcript = transcript.translate('en').fetch()
                    return final_transcript
        else:
            for i in transcript_list:
                start_with = str(i)[:2]
                if start_with == 'en':
                    final_transcript = YouTubeTranscriptApi.get_transcript(
                        v_id, languages=['en'])
                else:
                    transcript = transcript_list.find_transcript([start_with])
                    final_transcript = transcript.translate('en').fetch()
                return final_transcript

    except:

        final_transcript = "0"
        return None

!pip install googletrans
from googletrans import Translator


def g_translate(text, lang):

    translator = Translator()

    text_parts = text.split('. ')
    translated_text = []

    for parts in text_parts:
        translated_text.append(translator.translate(
            parts, src='en', dest=lang).text)

    return ' '.join(translated_text) + '.'

import re
def get_vid(youtube_url):
   # Sample YouTube URL
  # youtube_url = "https://www.youtube.com/watch?v=JxgmHe2NyeY&ab_channel=KrishNaik"
  # Define a regular expression pattern to extract the 'v' parameter
  pattern = r'v=([A-Za-z0-9_-]+)'
  # Use re.search to find the pattern in the URL
  match = re.search(pattern, youtube_url)
  if match:
      # Extract the video ID from the match object
      video_id = match.group(1)
      # print("YouTube Video ID:", video_id)
      return video_id
  else:
      print("YouTube Video ID not found in the URL.")
  return None

get_vid("https://www.youtube.com/watch?v=JxgmHe2NyeY&ab_channel=KrishNaik")

def summerize(youtube_url):
  vid = get_vid(youtube_url)
  trans = get_transcript_of_yt_video(vid)
  if (trans == None):
    print("there is no transcript for the video")
    return None
  transcript = ' '.join([t['text'] for t in trans])
  # print ("Summary of the video is : ")
  summary = gen_summary(transcript)
  # print("Length of the original transcript is ",len(transcript))
  # print("Length of the summary is ",len(summary))
  # print(len(transcript),len(summary))
  return summary
  # print(transcript)

youtube_url = "https://www.youtube.com/watch?v=JxgmHe2NyeY&ab_channel=KrishNaik"
sum = summerize(youtube_url)
print(sum)

!pip install pyngrok

from flask import Flask, jsonify, request
from pyngrok import ngrok

port_no = 3000

app = Flask(__name__)
ngrok.set_auth_token("2UWzHMWYiZJWJe4q2tjzjWHY2WS_3YUjr1z4cWsJKyLdH7Lr7")
public_url = ngrok.connect(port_no).public_url
@app.route('/summarize')
def video():
    # Get the 'link' parameter from the query string
    y_url = request.args.get('url')
    result = {"yrl": y_url }
    print(y_url)

    vid = get_vid(y_url)
    summary = summerize(y_url)
    res = {"summary ": summary}
    return jsonify(res), 200


print ("global link is ",public_url)

app.run(port = port_no)

